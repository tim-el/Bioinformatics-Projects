{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dependent-person",
   "metadata": {},
   "source": [
    "# HMM 2\n",
    "## Specification\n",
    "- Name: Template for Problem 19, 20, 21, 22 in Rosalind\n",
    "- Name your notebooks as: problem19.ipynb, problem20.ipynb, problem 22.ipynb\n",
    "- options: none\n",
    "- input: filename passed as first parameter to main\n",
    "- output: a text file. ( using print ( .... file=someFileObject) is a handy way to do this after you have opened someFileObject as a text file). I find it name to name these files by creating a string by concatenating the string named infile with \".out\" ... rosalind_ba10a.txt.out ( for example).\n",
    "-Rosalind Problem Names:\n",
    "    - Compute the Probability of a Hidden Path\n",
    "    - Compute the Probability of an Outcome Given a Hidden Path\n",
    "    - Implement the Viterbi Algorithm\n",
    "    - Compute the Probability of a String Emitted by an HMM\n",
    "\n",
    "As always, include an Inspection Intro Markdown that describes your specific algorithm at the beginning of the notebook, and another Inspection Results markdown at the end of the notebook that documents: your inspection team, the findings of the team, and your resolution of those findings.\n",
    "\n",
    "Please submit your notebooks, an example of one of the Rosalind files that you ran and passed, and the output that your program generated as a text file.\n",
    "\n",
    "## Description\n",
    "These are drawn from material presented in Ch. 10 of Compeau and Pevzner.\n",
    "\n",
    "We begin an exploration of models that provide us with inference into the hidden meaning lying within a sequence. Four HMM Rosalind examples provide a framework that will extend throughout the course. Note that these examples operate with probabilities rather than log-probabilities or log-odds scores. The concepts of the models are consistent, however.\n",
    "\n",
    "Please submit the rosalind input file that you tested with. This would be one of the \"Extra Datasets\" for each problem\n",
    "\n",
    "## Hints\n",
    "1) use of the python numpy module will simplify many of your operators, especially when higher precision math is required ( when products of many probabilities are calculated), or when vector operations are called for.\n",
    "\n",
    "2) Concepts presented in class will mirror those in the reading\n",
    "\n",
    "3) write your code for reusability. \n",
    "\n",
    "4) Consider your data structures carefully. You will likely need transition and emission matrices. Carefully consider how to address these. For those who are comfortable with dot product and matrix multiplication, these can dramatically simplify your code.\n",
    "\n",
    "Here is a template to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-management",
   "metadata": {},
   "source": [
    "## Inspection Intro\n",
    "\n",
    "- The BioinformaticsModel class encapsulates the functionality for parsing input data and calculating emission probabilities using forward probabilities. The class is designed to maintain state and provide a clean interface for these calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "photographic-vietnamese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A B\n",
      "A 1.0 0.0\n",
      "B 1.0 0.0\n",
      "--------\n",
      "  x y z\n",
      "A 0.41414141414141414 0.29292929292929293 0.29292929292929293\n",
      "B 1.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# python3\n",
    "import sys\n",
    "from math import *\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Implement Viterbi learning for estimating the parameters of an HMM.\n",
    "     Input: A number of iterations j, followed by a string x of symbols emitted by an HMM,\n",
    "     followed by the HMM's alphabet Σ, followed by the HMM's states, followed by initial transition\n",
    "     and emission matrices for the HMM.\n",
    "     Output: Emission and transition matrices resulting from applying Viterbi learning for j iterations.\n",
    "\n",
    "Sample Input:\n",
    "100\n",
    "--------\n",
    "zyzxzxxxzz\n",
    "--------\n",
    "x y z\n",
    "--------\n",
    "A B\n",
    "--------\n",
    "\tA\tB\n",
    "A\t0.599\t0.401\t\n",
    "B\t0.294\t0.706\t\n",
    "--------\n",
    "\tx\ty\tz\n",
    "A\t0.424\t0.367\t0.209\t\n",
    "B\t0.262\t0.449\t0.289\n",
    "Sample Output:\n",
    "\tA\tB\n",
    "A\t0.5\t0.5\t\n",
    "B\t0.0\t1.0\t\n",
    "--------\n",
    "\tx\ty\tz\n",
    "A\t0.333\t0.333\t0.333\t\n",
    "B\t0.4\t0.1\t0.5\n",
    "\n",
    "HMM Parameter Learning Problem: Estimate the parameters of an HMM explaining an emitted string.\n",
    "     Input: A string x = x1 ... xn emitted by an HMM with unknown transition and emission probabilities.\n",
    "     Output: A transition matrix Transition and an emission matrix Emission that maximize\n",
    "     Pr(x, π) over all possible transition and emission matrices and over all hidden paths π.\n",
    "\n",
    "Unfortunately, the HMM Parameter Learning Problem is intractable, and so we will instead develop a heuristic that is \n",
    "analogous to the Lloyd algorithm for k-means clustering. In that algorithm, we iterated two steps, “From Centers to \n",
    "Clusters”,\n",
    "\n",
    "(Data, ?, Centers) → HiddenVector,\n",
    "\n",
    "and “From Clusters to Centers”,\n",
    "\n",
    "(Data, HiddenVector, ?) → Centers.\n",
    "\n",
    "As for HMM parameter estimation, we begin with an initial random guess for Parameters. Then, we use the Viterbi algorithm \n",
    "to find the optimal hidden path π:\n",
    "\n",
    "(x, ?, Parameters) → π\n",
    "\n",
    "Once we know π, we will question our original choice of Parameters and apply our solution to the HMM Parameter Estimation \n",
    "Problem to update Parameters based on x and π:\n",
    "\n",
    "(x, π, ?) → Parameters′\n",
    "\n",
    "We then iterate over these two steps, hoping that the estimated parameters are getting closer and closer to the parameters \n",
    "solving the HMM Parameter Learning Problem:\n",
    "\n",
    "(x, ?, Parameters) → (x, π, Parameters) → (x, π, ?) → (x, π, Parameters′) → (x, ?, Parameters′) → (x, π′, Parameters′) → \n",
    "(x, π′, ?) → (x, π′, Parameters′′) → . . .\n",
    "\n",
    "This approach to learning the HMM’s parameters is called Viterbi learning.\n",
    "'''\n",
    "\n",
    "class ViterbiLearning:\n",
    "    def __init__(self):\n",
    "        x, transition, emission, alphabet, states, iterNo = self.readFromFile()\n",
    "        transition, emission = self.viterbiLearning(x, transition, emission, alphabet, states, iterNo)\n",
    "        self.saveTransitionAndEmission(alphabet, states, transition, emission)\n",
    "\n",
    "    def readFromFile(self):\n",
    "        f = open('rosalind_ba10i.txt', 'r')\n",
    "        data = f.read().strip().split()\n",
    "        iterNo = int(data[0])\n",
    "        x = data[2]\n",
    "        ind = [i for i in range(len(data)) if '--------' == data[i]]\n",
    "        alphabet = data[ind[1]+1:ind[2]]\n",
    "        states = data[ind[2]+1:ind[3]]\n",
    "        transition = np.zeros((len(states), len(states)), dtype = float)\n",
    "        emission = np.zeros((len(states), len(alphabet)), dtype = float)\n",
    "        for i in range(len(states)):\n",
    "            transition[i, :] = [float(d) for d in data[ind[3]+len(states)+2+i*(len(states)+1):ind[3]+len(states)+1+(i+1)*(len(states)+1)]]\n",
    "            emission[i, :] = [float(d) for d in data[ind[4]+len(alphabet)+2+i*(len(alphabet)+1):ind[4]+len(alphabet)+1+(i+1)*(len(alphabet)+1)]]\n",
    "        return x, transition, emission, alphabet, states, iterNo\n",
    "    \n",
    "    def decode(self, xList, transition, emission):\n",
    "        n, l = len(xList), transition.shape[0]\n",
    "        score = [[1. for _ in range(l)] for __ in range(n)]\n",
    "        backtrack = [[None for _ in range(l)] for __ in range(n)]\n",
    "        for k in range(l):\n",
    "            score[0][k] = emission[k, xList[0]]/l\n",
    "        for i in range(1, n):\n",
    "            for k in range(l):\n",
    "                currScore = [score[i-1][kpre] * transition[kpre, k] * emission[k, xList[i]] for kpre in range(l)]\n",
    "                ind = np.argmax(currScore)\n",
    "                backtrack[i][k] = ind\n",
    "                score[i][k] = currScore[ind]\n",
    "        currState = np.argmax(score[n-1])\n",
    "        pathList = [currState]\n",
    "        for i in range(n-1, 0, -1):\n",
    "            currState = backtrack[i][currState]\n",
    "            pathList.insert(0, currState)\n",
    "        return pathList\n",
    "\n",
    "    def estimateParameters(self, xList, pathList, transition, emission):\n",
    "        transition = np.zeros_like(transition, dtype = float)\n",
    "        emission = np.zeros_like(emission, dtype = float)\n",
    "        n, l = emission.shape\n",
    "        for i in range(len(pathList)-1):\n",
    "            transition[pathList[i], pathList[i+1]] += 1\n",
    "        for i in range(len(xList)):\n",
    "            emission[pathList[i], xList[i]] += 1\n",
    "        for i in range(n):\n",
    "            sum1 = sum(transition[i,:])\n",
    "            if 0 == sum1:\n",
    "                transition[i,:] += 1/n\n",
    "            else:\n",
    "                transition[i,:] /= sum1\n",
    "            sum2 = sum(emission[i,:])\n",
    "            if 0 == sum2:\n",
    "                emission[i,:] += 1/l\n",
    "            else:\n",
    "                emission[i,:] /= sum2\n",
    "        return transition, emission\n",
    "\n",
    "    def viterbiLearning(self, x, transition, emission, alphabet, states, iterNo):\n",
    "        x2ind, ind2x = {alphabet[i]:i for i in range(len(alphabet))}, {i:alphabet[i] for i in range(len(alphabet))}\n",
    "        path2ind, ind2path = {states[i]:i for i in range(len(states))}, {i:states[i] for i in range(len(states))}\n",
    "        xList = [x2ind[x[i]] for i in range(len(x))]\n",
    "        for _ in range(iterNo):\n",
    "            pathList = self.decode(xList, transition, emission)\n",
    "            transition, emission = self.estimateParameters(xList, pathList, transition, emission)\n",
    "        return transition, emission\n",
    "    \n",
    "    def saveTransitionAndEmission(self, alphabet, states, fullTransition, emission):\n",
    "        f = open('rosalind_ba10i_out.txt', 'w')\n",
    "        print(' '.join([' '] + states))\n",
    "        f.write('\\t'+'\\t'.join(states) + '\\n')\n",
    "        for i in range(fullTransition.shape[0]):\n",
    "            print(' '.join([states[i]] + list(map(str, fullTransition[i, :]))))\n",
    "            f.write('\\t'.join([states[i]] + list(map(str, fullTransition[i, :])))+'\\n')\n",
    "        print('--------')\n",
    "        f.write('--------'+'\\n')\n",
    "        print(' '.join([' '] + alphabet))\n",
    "        f.write('\\t'+'\\t'.join(alphabet)+'\\n')\n",
    "        for i in range(emission.shape[0]):\n",
    "            print(' '.join([states[i]] + list(map(str, emission[i, :]))))\n",
    "            f.write('\\t'.join([states[i]] + list(map(str, emission[i, :])))+'\\n')\n",
    "        f.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ViterbiLearning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-warning",
   "metadata": {},
   "source": [
    "## Inspection Results\n",
    "\n",
    "# Q: Why does the calculate_emission_probability method use a two-dimensional list for forward_probabilities? (Connor)\n",
    "- A: The method uses a two-dimensional list to store the forward probabilities at each step for each state. This structure is essential for implementing the dynamic programming approach to calculate the total probability efficiently.\n",
    "\n",
    "# Q: How does the BioinformaticsModel class manage the complexity of handling multiple matrices (transition and emission)? (Ivana)\n",
    "- A: The class uses dictionaries to store both transition and emission matrices, facilitating easy access and manipulation of these complex data structures within the class methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
